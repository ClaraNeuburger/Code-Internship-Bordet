{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610adee2",
   "metadata": {},
   "source": [
    "The following code serves as a representation of the workflow followed during this research internship. \n",
    "\n",
    "The goal is to exploit a dataset of full body PET MRI (T2 and dixon sequences) and CT to build and train a model which can create synthetic CT from the MRI images. The goal then using these sCT is to use them for body composition:  https://github.com/UMEssen/Body-and-Organ-Analysis\n",
    "\n",
    "Body composition is a biomarker which can be used to determine treatment plans in oncology, cardiology. The CT image is fed to a software which will separate it into different regions thanks to a model that thresholds the HU to a specific intensity range. The regions to be determine are : \n",
    "- Subcutaneous adipose tissue\n",
    "- Total adipose tissue\n",
    "- Visceral adipose tissue\n",
    "- Muscle volume\n",
    "\n",
    "The software used also allows an organ segmentation of the trunk whiwh can also be later tested with the sCT. \n",
    "\n",
    "The advantage of creating such a sCT is to be able to be able to produce this body composition report without the need for an irradiating scan. Past models for creating sCT at the Bordet Institute were intended for radiotherapy treatments. The main difference here is that there is no need for such precision for body composition because we are more interested in global composition then the exact position of every organ in the scan. \n",
    "\n",
    "From the time being, the data is not yet recovered, the following code was used to explore torchio functionalities and learn how to load medical data into a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4638f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radiology/anaconda3/envs/InternshipProject/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch \n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader\n",
    "import pydicom\n",
    "import os\n",
    "import numpy as np\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf28c91",
   "metadata": {},
   "source": [
    "### Load the data using torchio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dcc77",
   "metadata": {},
   "source": [
    "It is often recommended to use NIfTI format for managing medical images, why? what are the advantages compared with simply working with dicom format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea10d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will randomly split my data into 2 categories: training and testing \n",
    "# The validation set will be taken directly from the training set\n",
    "def random_split(subjects, ratio=0.9):\n",
    "    num_subjects = len(subjects)\n",
    "    num_training_subjects = int(ratio * num_subjects)\n",
    "    num_test_subjects = num_subjects - num_training_subjects\n",
    "\n",
    "    num_split_subjects = num_training_subjects, num_test_subjects\n",
    "    return torch.utils.data.random_split(subjects, num_split_subjects)\n",
    "\n",
    "\n",
    "\n",
    "def Create_dataset(rootdir):\n",
    "    file_paths = []\n",
    "    subject_paths = []\n",
    "    subjects_list = []\n",
    "    nb_subjects = 0\n",
    "    \n",
    "    # Start by recovering the path to the subjects\n",
    "    for subjects in os.listdir(rootdir):\n",
    "        subject_path = os.path.join(rootdir,subjects)\n",
    "        subject_paths.append(subject_path)\n",
    "        nb_subjects += 1\n",
    "    \n",
    "   \n",
    "    for subject_path in subject_paths:\n",
    "        # Recover all the files\n",
    "        ct_path = None\n",
    "        mri_path = None\n",
    "        \n",
    "        for subfolder in os.listdir(subject_path):\n",
    "            subfolder_path= os.path.join(subject_path,subfolder)\n",
    "            if subfolder.startswith('CT'):\n",
    "                ct_path = subfolder_path\n",
    "            elif subfolder.startswith('IRM'):\n",
    "                mri_path = subfolder_path\n",
    "        \n",
    "            \n",
    "        # Create the subject format of torchio witht the scans and the id of the patient\n",
    "        sub = tio.Subject(\n",
    "            CT = tio.ScalarImage(ct_path),\n",
    "            MRI = tio.ScalarImage(mri_path),\n",
    "            id_patient = os.path.basename(subject_path),\n",
    "\n",
    "        ) \n",
    "        \n",
    "        subjects_list.append(sub)\n",
    "        \n",
    "    # List of transforms : for training, possibility of data augmentation!\n",
    "    # Careful when rescaling intensity with multiple images (t1,t2...), use include to select one type of image\n",
    "    transforms_train = [\n",
    "        tio.ToCanonical(),\n",
    "        tio.Clamp(out_min=0,out_max = 2500),\n",
    "        #tio.RescaleIntensity(out_min_max=(0,1)),\n",
    "        tio.RescaleIntensity(out_min_max=(0, 1), include=(['CT'])),\n",
    "        tio.RescaleIntensity(out_min_max=(0, 1), include=(['IRM'])),\n",
    "        tio.RandomFlip(p=0.2), #data augmentation\n",
    "        tio.RandomAffine(scales=(0.9, 1.2), degrees=15, p=0.2),  #data augmentation\n",
    "    ]\n",
    "    \n",
    "    transforms_test = [\n",
    "        tio.ToCanonical(),\n",
    "        tio.Clamp(out_min=0,out_max = 2500),\n",
    "        tio.RescaleIntensity(out_min_max=(0,1)),\n",
    "    ]\n",
    "\n",
    "    # Create our own set of tranforms that we will apply to our dataset\n",
    "    transfo_tr = tio.Compose(transforms_train)\n",
    "    transfo_te = tio.Compose(transforms_test)\n",
    "    \n",
    "    \n",
    "    # Random split of the subjects with a defined ratio\n",
    "    train_val_subjects, testing_subjects = random_split(subjects_list, ratio=0.9)\n",
    "    training_subjects, validation_subjects = random_split(train_val_subjects, ratio = 0.9)\n",
    "    \n",
    "    # Create the final datasets made from the different subjects\n",
    "    training_dataset = tio.SubjectsDataset(training_subjects, transform=transfo_tr)\n",
    "    validation_dataset = tio.SubjectsDataset(validation_subjects, transform = transfo_tr)\n",
    "    testing_dataset = tio.SubjectsDataset(testing_subjects, transform = transfo_te)\n",
    "\n",
    "    \n",
    "    dataset = tio.SubjectsDataset(subjects_list, transform = transfo_tr)\n",
    "    #print('Dataset successfully created! The set is made of',nb_subjects,'patients.')\n",
    "    print('Datasets successfully created!')\n",
    "    print('=======================================================')\n",
    "    print(' Training set is made of', len(training_dataset), 'images \\n Validation set is made of', len(validation_dataset), 'images \\n', 'Testing set is made of', len(testing_dataset), 'images')\n",
    "    print('=======================================================')\n",
    "    ids = [subject.id_patient for subject in dataset.dry_iter()]\n",
    "    \n",
    "    return dataset\n",
    "    #return training_dataset, validation_dataset, testing_dataset, file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f627c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets successfully created!\n",
      "=======================================================\n",
      " Dataset is made of 1 images\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "#training_dataset, validation_dataset, testing_dataset, file_paths = Create_dataset('../Clara intern/whole body')\n",
    "\n",
    "dataset = Create_dataset('../Clara intern/whole body')\n",
    "\n",
    "# The plot of a Subject object will build the coronal and axial view when we only send it the sagittal view\n",
    "#dataset[0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f251dc26",
   "metadata": {},
   "source": [
    "Now that the dataset is crated with 3D images for each subject, we need to decide how to exploit them: do we keep them 3D in the model or do we explore them slice by slice to achieve a 2D analysis?\n",
    "\n",
    "The following part will explore how this dataset can be included and preprocessed before feeding it to the model. \n",
    "It is important to specify that with the real data, some pre processing steps might be done using mice software to re-align the PET MRI with the CT (make sure they have the same zero and that morphological differences are not too disturbing).\n",
    "\n",
    "A crucial notion to this project is to make sure that the model and preprocessing steps chosen will match the concrete clinical applications. \n",
    "\n",
    "The issue with medical images is their quantity, they often contain hundreds of milions of voxels and cannot always be downsampled. \n",
    "Big differences are to be noted when working with medical images: their size, the fact that they might be 3D, their format (often DICOM which contains metadata about the patient), the fact that they cannot be easily downsampled if details are needed. \n",
    "\n",
    "The batch size of medical images tend to be way smaller than the usual ones because of the quantity of information contained in a single medical image. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51b103",
   "metadata": {},
   "source": [
    "### Create patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd51e9a",
   "metadata": {},
   "source": [
    "To train in 2D, one need to extract slices from 3D volumes, aggrefating the inference results to generate a 3D volume: this is called batch based training, the patches along a dimension is one (cite Torchio patch based pipeline). \n",
    "\n",
    "In torchio it is possible to use patch samplers: functions that will randomly extract pathces from volumes when fed a SubjectDataset like we created earlier. \n",
    "We tend to use batch sampling when working with medical images because of their size: working with smaller patches reduce computation. It has also been proven that soemtimes, algorithms using patches can be more efficient, it is the case for denoising for example. \n",
    "\n",
    "You could chose Uniform or Weighted patching: uniform will take random patches from a volume with a uniform probabiliy while the weighted sampler will randomly extract patches given a probability map. \n",
    "If you chose very small patches, it could be intersting to create a probability map for each slice in order to focus on the region of interest and not consider much the background. \n",
    "However with larger patches, a uniform sampler is easier to use. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f559f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIFORM sampler\n",
    "# Chose the sampler for your data\n",
    "patch_size = (64,64,8)\n",
    "sampler_uniform = tio.data.UniformSampler(patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d72c4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHTED sampler \n",
    "\n",
    "# Constantes \n",
    "probability_map = []\n",
    "SIZE_IMAGE = 256\n",
    "threshold = 0.5\n",
    "\n",
    "\n",
    "# For each slice, create a matching array that takes 1 when the value is not background \n",
    "slices = [pydicom.read_file(images_dir) for images_dir in file_paths]\n",
    "for slice in slices:\n",
    "    data_slice = apply_voi_lut(slice.pixel_array, slice)\n",
    "    proba_map = np.zeros((SIZE_IMAGE,SIZE_IMAGE))\n",
    "    proba_map[data_slice > threshold] = 1\n",
    "    probability_map.append(proba_map)\n",
    "\n",
    "    \n",
    "# Create a stack of these 2D arrays to feed them to the sampler\n",
    "probability_map_stack =np.stack(probability_map, axis=0)\n",
    "\n",
    "sampler = tio.data.WeightedSampler(patch_size=(32, 32, 5), probability_map = probability_map_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "581b0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_volume=4\n",
    "max_length=200 \n",
    "num_workers=8\n",
    "\n",
    "\n",
    "patches_training_set = tio.Queue(\n",
    "        subjects_dataset=training_dataset,\n",
    "        max_length=max_length, # Maximum number of patches that can be stored in the queue\n",
    "        samples_per_volume=samples_per_volume, # Number if patches to be extracted from each volume\n",
    "        sampler=sampler, # The sampler that we defined previously\n",
    "        num_workers=num_workers, # Number of subprocesses to use for data loading\n",
    "        shuffle_subjects=True,\n",
    "        shuffle_patches=True,\n",
    "    )\n",
    "\n",
    "patches_validation_set = tio.Queue(\n",
    "    subjects_dataset=validation_dataset,\n",
    "    max_length=max_length,\n",
    "    samples_per_volume=1,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "# Define loader, no need to shuffle it as the queue already does it\n",
    "training_loader_patches = DataLoader(\n",
    "    patches_training_set,\n",
    "    batch_size = 16,\n",
    "    num_workers = 0, # Need to instianciate it to 0 for Loaders\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "validation_loader_patches = DataLoader(\n",
    "    patches_validation_set,\n",
    "    batch_size = 16,\n",
    "    num_workers = 0, # Need to instianciate it to 0 for Loaders\n",
    "    drop_last = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0118eb8",
   "metadata": {},
   "source": [
    "### Definition of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cc3f0",
   "metadata": {},
   "source": [
    "In the following section we will define the model that we will later train. As the scans are considered paired, the model used is a Residual Neural Network which is an architecture in which the weight layers learn residual functions with reference to the layer inputs.\n",
    "\n",
    "The following code is adapted from a 2D ResNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f319ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#import network as network\n",
    "import network_ResNet as network\n",
    "from matplotlib import pyplot as plt\n",
    "from util import print_log, format_train_log, format_validation_log\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, expr_dir, seed=None, batch_size=None,\n",
    "                 epoch_count=1, niter=150, niter_decay=50, beta1=0.5, \n",
    "                 lr=0.0003, ngf=64, n_blocks=9, input_nc=1, output_nc=1, \n",
    "                 use_dropout=True, norm='batch', max_grad_norm=500.,\n",
    "                 monitor_grad_norm=True, save_epoch_freq=10, print_freq=15, \n",
    "                 display_epoch_freq=1, testing=False, resume=False):\n",
    "\n",
    "        self.expr_dir = expr_dir\n",
    "        self.seed = seed\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.epoch_count = epoch_count\n",
    "        self.niter = niter\n",
    "        self.niter_decay = niter_decay\n",
    "        self.beta1 = beta1\n",
    "        self.lr = lr\n",
    "        self.old_lr = self.lr\n",
    "\n",
    "        self.ngf = ngf\n",
    "        self.n_blocks = n_blocks\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.use_dropout = use_dropout\n",
    "        self.norm = norm\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.monitor_grad_norm = monitor_grad_norm\n",
    "        self.save_epoch_freq = save_epoch_freq\n",
    "        self.print_freq = print_freq\n",
    "        self.display_epoch_freq = display_epoch_freq\n",
    "        self.time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        # define network we need here\n",
    "        self.netG = network.define_generator(input_nc=self.input_nc, \n",
    "                                             output_nc=self.output_nc, \n",
    "                                             ngf=self.ngf,\n",
    "                                             n_blocks=self.n_blocks, \n",
    "                                             use_dropout=self.use_dropout,\n",
    "                                             device=self.device)\n",
    "\n",
    "        # define all optimizers here\n",
    "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
    "                                            lr=self.lr, \n",
    "                                            betas=(self.beta1, 0.999),\n",
    "                                            )\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        self.resume = resume\n",
    "\n",
    "        if not os.path.exists(expr_dir):\n",
    "            os.makedirs(expr_dir)\n",
    "\n",
    "        if not os.path.exists(os.path.join(expr_dir, 'TensorBoard')):\n",
    "            os.makedirs(os.path.join(expr_dir, 'TensorBoard', self.time))\n",
    "\n",
    "        if not os.path.exists(os.path.join(expr_dir, 'TensorBoard', self.time, 'training_visuals')):\n",
    "            os.makedirs(os.path.join(expr_dir, 'TensorBoard', self.time, 'training_visuals'))\n",
    "\n",
    "        if not os.path.exists(os.path.join(expr_dir, 'TensorBoard', self.time, 'testing_visuals')):\n",
    "            os.makedirs(os.path.join(expr_dir, 'TensorBoard', self.time, 'testing_visuals'))\n",
    "\n",
    "        if not testing:\n",
    "            num_params = 0\n",
    "            with open(\"%s/nets.txt\" % self.expr_dir, 'w') as nets_f:\n",
    "                num_params += network.print_network(self.netG, nets_f)\n",
    "                nets_f.write('# parameters: %d\\n' % num_params)\n",
    "                nets_f.flush()\n",
    "\n",
    "        if resume:\n",
    "            self.load(os.path.join(self.expr_dir, \"latest\"), True)\n",
    "            self.netG.to(self.device)\n",
    "\n",
    "    def train(self, train_dataset, validation_set):\n",
    "        self.batch_size = train_dataset.batch_size\n",
    "        self.save_options()\n",
    "        out_f = open(f\"{self.expr_dir}/results.txt\", 'w')\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "\n",
    "        tensorboard_writer = SummaryWriter(os.path.join(self.expr_dir, 'TensorBoard', self.time))\n",
    "\n",
    "        if self.seed is not None:\n",
    "            print(f\"using random seed: {self.seed}\")\n",
    "            random.seed(self.seed)\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "            if use_gpu:\n",
    "                torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "        total_steps = 0\n",
    "        print_start_time = time.time()\n",
    "\n",
    "        for epoch in range(self.epoch_count, self.niter + self.niter_decay + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            epoch_iter = 0\n",
    "            total_loss = 0\n",
    "            for data in train_dataset:\n",
    "                b0_guidance = data['b0guidance'][tio.DATA].to(self.device)\n",
    "                b0_guidance = b0_guidance.transpose_(2, 4)\n",
    "                b0_guidance = torch.squeeze(b0_guidance, dim=2)\n",
    "                b800_noisy = data['b800noisy'][tio.DATA].to(self.device)\n",
    "                b800_noisy = b800_noisy.transpose_(2, 4)\n",
    "                b800_noisy = torch.squeeze(b800_noisy, dim=2)\n",
    "                b800_reference = data['b800reference'][tio.DATA].to(self.device)\n",
    "                b800_reference = b800_reference.transpose_(2, 4)\n",
    "                b800_reference = torch.squeeze(b800_reference, dim=2)\n",
    "                total_steps += self.batch_size\n",
    "                epoch_iter += self.batch_size\n",
    "                \n",
    "                if self.monitor_grad_norm:\n",
    "                    losses, visuals, _ = self.train_instance(b0_guidance, b800_noisy, b800_reference)\n",
    "                else:\n",
    "                    losses, visuals = self.train_instance(b0_guidance, b800_noisy, b800_reference)\n",
    "                \n",
    "                total_loss += losses['Loss']\n",
    "                    \n",
    "            loss = total_loss/len(train_dataset)\n",
    "                \n",
    "            if total_steps % self.print_freq == 0:\n",
    "                t = (time.time() - print_start_time) / self.batch_size\n",
    "                print_log(out_f, format_train_log(epoch, epoch_iter, losses, t))\n",
    "                tensorboard_writer.add_scalars('Loss', {'train': loss}, total_steps)\n",
    "                print_start_time = time.time()\n",
    "\n",
    "            print_start_time = time.time()\n",
    "            \n",
    "            if epoch % self.display_epoch_freq == 0:\n",
    "                self.visualize(visuals, data['b0guidance'][tio.AFFINE], epoch, \n",
    "                               epoch_iter / self.batch_size)\n",
    "\n",
    "            if epoch % self.save_epoch_freq == 0:\n",
    "                print_log(out_f, 'saving the model at the end of epoch %d, iterations %d' % (epoch, total_steps))\n",
    "                self.save('latest')\n",
    "\n",
    "                losses_validation, visuals_validation, affine = self.validation(validation_set)\n",
    "\n",
    "                self.visualize(visuals_validation, affine, epoch, epoch_iter / self.batch_size, \"testing\")\n",
    "                t = (time.time() - print_start_time) / self.batch_size\n",
    "                print_log(out_f, format_validation_log(epoch, epoch_iter, losses_validation, t))\n",
    "\n",
    "                tensorboard_writer.add_scalars('Loss', {'Test': losses_validation['Loss']},\n",
    "                                               total_steps)\n",
    "            print_log(out_f, 'End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "                      (epoch, self.niter + self.niter_decay, time.time() - epoch_start_time))\n",
    "\n",
    "            if epoch > self.niter:\n",
    "                self.update_learning_rate()\n",
    "\n",
    "        out_f.close()\n",
    "        tensorboard_writer.close()\n",
    "\n",
    "\n",
    "    def train_instance(self, b0_guidance, b800_noisy, b800_reference):\n",
    "        \n",
    "        inpt = torch.cat((b800_noisy, b0_guidance), dim=1)\n",
    "        b800_denoised = self.netG.forward(inpt)\n",
    "\n",
    "        self.optimizer_G.zero_grad()\n",
    "        loss = self.loss(b800_denoised, b800_reference)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.netG.parameters(), \n",
    "                                                   self.max_grad_norm)\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        losses = OrderedDict([('Loss', loss.data.item())])\n",
    "        visuals = OrderedDict([('Noisy_b800', b800_noisy.data),\n",
    "                               ('Denoisy_b800', b800_denoised.data),\n",
    "                               ('Reference_b800', b800_reference.data)\n",
    "                               ])\n",
    "        if self.monitor_grad_norm:\n",
    "            grad_norm = OrderedDict([('grad_norm', grad_norm)])\n",
    "\n",
    "            return losses, visuals, grad_norm\n",
    "\n",
    "        return losses, visuals\n",
    "    \n",
    "    \n",
    "    \n",
    "    def validation(self, validation_set):\n",
    "        \n",
    "        self.netG.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in validation_set:\n",
    "                \n",
    "                b0_guidance = data['b0guidance'][tio.DATA].to(self.device)\n",
    "                b0_guidance = b0_guidance.transpose_(2, 4)\n",
    "                b0_guidance = torch.squeeze(b0_guidance, dim=2)\n",
    "                b800_noisy = data['b800noisy'][tio.DATA].to(self.device)\n",
    "                b800_noisy = b800_noisy.transpose_(2, 4)\n",
    "                b800_noisy = torch.squeeze(b800_noisy, dim=2)\n",
    "                b800_reference = data['b800reference'][tio.DATA].to(self.device)\n",
    "                b800_reference = b800_reference.transpose_(2, 4)\n",
    "                b800_reference = torch.squeeze(b800_reference, dim=2)\n",
    "                \n",
    "                inpt = torch.cat((b800_noisy, b0_guidance), dim=1)\n",
    "                b800_denoised = self.netG.forward(inpt)\n",
    "                \n",
    "                loss_val = self.loss(b800_denoised, b800_reference)\n",
    "                total_loss += loss_val.item()\n",
    "\n",
    "        total_loss = total_loss / len(validation_set)\n",
    "        \n",
    "        losses = OrderedDict([('Loss', total_loss)])\n",
    "        visuals = OrderedDict([('Noisy_b800', b800_noisy.data),\n",
    "                               ('Denoisy_b800', b800_denoised.data),\n",
    "                               ('Reference_b800', b800_reference.data),\n",
    "                               ])\n",
    "        \n",
    "        return losses, visuals, data['b0guidance'][tio.AFFINE]\n",
    "\n",
    "\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        lrd = self.lr / self.niter_decay\n",
    "        lr = self.old_lr - lrd\n",
    "        for param_group in self.optimizer_G.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        print('update learning rate: %f -> %f' % (self.old_lr, lr))\n",
    "        self.old_lr = lr\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, checkpoint_name):\n",
    "        checkpoint_path = os.path.join(self.expr_dir, checkpoint_name)\n",
    "        checkpoint = {\n",
    "            'netG': self.netG.state_dict(),\n",
    "            'optimizer_G': self.optimizer_G.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def visualize(self, visuals, affine, epoch, index, state=\"training\"):\n",
    "        b800_noisy = visuals['Noisy_b800'].cpu().transpose_(1, 3)\n",
    "        b800_denoised = visuals['Denoisy_b800'].cpu().transpose_(1, 3)\n",
    "        b800_reference = visuals['Reference_b800'].cpu().transpose_(1, 3)\n",
    "        \n",
    "        b800_noisy = b800_noisy[:,None,:,:,:]\n",
    "        b800_denoised = b800_denoised[:,None,:,:,:]\n",
    "        b800_reference = b800_reference[:,None,:,:,:]\n",
    "        \n",
    "        for i in range(1):\n",
    "            subject = tio.Subject(\n",
    "                Noisy=tio.ScalarImage(tensor=b800_noisy[i], affine=affine[i]),\n",
    "                Denoisy=tio.ScalarImage(tensor=b800_denoised[i], affine=affine[i]),\n",
    "                Reference=tio.ScalarImage(tensor=b800_reference[i], affine=affine[i]),\n",
    "            )\n",
    "\n",
    "            save_path = os.path.join(self.expr_dir, 'TensorBoard', self.time, state + \"_visuals\")\n",
    "            save_path = os.path.join(save_path, 'cycle_' + str(epoch) + '_' + str(index) + '_' + str(\n",
    "                i) + '.png')\n",
    "            subject.plot(show=False, output_path=save_path)\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "    def load(self, checkpoint_path, optimizer=False):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        #checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        self.netG.load_state_dict(checkpoint['netG'])\n",
    "\n",
    "        if optimizer:\n",
    "            self.optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self):\n",
    "        self.netG.eval()\n",
    "        \n",
    "        \n",
    "    def inferencePatch(self, dataset, export_path=None, checkpoint=None, save=False):\n",
    "        \n",
    "        checkpoint = checkpoint or os.path.join(self.expr_dir, \"latest\")\n",
    "        \n",
    "        self.load(checkpoint)\n",
    "        self.eval()\n",
    "\n",
    "        subjects = []\n",
    "        for n,subject in enumerate(dataset.subject):\n",
    "            \n",
    "            grid_sampler = tio.inference.GridSampler(\n",
    "                subject,\n",
    "                dataset.patch_size,\n",
    "                dataset.patch_overlap)\n",
    "            \n",
    "            aggregator = tio.inference.GridAggregator(grid_sampler, \n",
    "                                                      overlap_mode='hann')\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                grid_sampler, batch_size=1, drop_last=True)\n",
    "            \n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(loader):\n",
    "                    \n",
    "                    b0_guidance = data['b0guidance'][tio.DATA].to(self.device)\n",
    "                    b0_guidance = b0_guidance.transpose_(2, 4)\n",
    "                    b0_guidance = torch.squeeze(b0_guidance, dim=2)\n",
    "                    locations = data[tio.LOCATION]\n",
    "                    b800_noisy = data['b800noisy'][tio.DATA].to(self.device)\n",
    "                    b800_noisy = b800_noisy.transpose_(2, 4)\n",
    "                    b800_noisy = torch.squeeze(b800_noisy, dim=2)\n",
    "                    \n",
    "                    inpt = torch.cat((b800_noisy, b0_guidance), dim=1)\n",
    "                    b800_denoised = self.netG.forward(inpt)\n",
    "                    \n",
    "                    b800_denoised = b800_denoised[:,:,None,:,:]\n",
    "                    b800_denoised = b800_denoised.transpose_(2, 4)\n",
    "                                    \n",
    "                    aggregator.add_batch(b800_denoised, locations)\n",
    "                    \n",
    "                    print(f\"patch {i + 1}/{len(loader)}\")\n",
    "                    \n",
    "                foreground = aggregator.get_output_tensor()\n",
    "                affine = torch.squeeze(data['b0guidance'][tio.AFFINE],dim=0)\n",
    "                b0_guidance = data['b0guidance'][tio.DATA].to(self.device)\n",
    "                                \n",
    "                subject = tio.Subject(\n",
    "                    b800denoisy = tio.ScalarImage(tensor=foreground, affine=affine)\n",
    "                    )\n",
    "                    \n",
    "                print(f\"{time.time() - start} sec. for evaluation\")\n",
    "                subjects.append(subject)\n",
    "            \n",
    "        return subjects\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
